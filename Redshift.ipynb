{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBilding a relational data model\\nConnect to athena and query data\\nBuild ETL Jobs\\nSave Results to s3 Buckets\\nGlue Deployment\\nBuild data models on redshift\\nCopy data to redshift\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bilding a relational data model\n",
    "Connect to athena and query data\n",
    "Build ETL Jobs\n",
    "Save Results to s3 Buckets\n",
    "Glue Deployment\n",
    "Build data models on redshift\n",
    "Copy data to redshift\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd \n",
    "from io import StringIO\n",
    "import time\n",
    "import awswrangler as wr  \n",
    "import redshift_connector\n",
    "import psycopg2\n",
    "import json\n",
    "import sqlalchemy\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new-s3-redshift'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('cluster.config'))\n",
    "config.get(\"DWH\",\"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('youtube-api-2023', 'new-s3-redshift')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AWS Credentials\n",
    "AWS_Key = config.get('AWS','Key')\n",
    "AWS_Secret_Key = config.get('AWS','Secret_Key')\n",
    "\n",
    "# Data WareHouse Info\n",
    "DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DWH_PORT\")\n",
    "DWH_IAM_ROLE_NAME = config.get(\"DWH\",\"DWH_IAM_ROLE_NAME\")\n",
    "DWH_END_POINT = config.get(\"DWH\",\"DWH_END_POINT\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER,DWH_IAM_ROLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting Redshift,IAM& s3 to AWS\n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3_Buckets = boto3.resource('s3',\n",
    "                            region_name=\"us-east-1\")\n",
    "\n",
    "IAM = boto3.client('iam',\n",
    "                   region_name=\"us-east-1\")\n",
    "\n",
    "ec2 = boto3.client('ec2',\n",
    "                        region_name = \"us-east-1\")\n",
    "\n",
    "Redshift = boto3.client('redshift',\n",
    "                        region_name = \"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::807724986505:role/new-s3-redshift'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating IAM Role for the Redshift Access to the s3 Bucket\n",
    "RoleARN = IAM.get_role(RoleName = DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "RoleARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "# Creating Redshift Cluster \n",
    "try:\n",
    "    response = Redshift.create_cluster(\n",
    "    ClusterType = DWH_CLUSTER_TYPE,\n",
    "    NodeType = DWH_NODE_TYPE,\n",
    "        \n",
    "    # Identifiers & Credentials\n",
    "    DBName = DWH_DB,\n",
    "    ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "    MasterUsername = DWH_DB_USER,\n",
    "    MasterUserPassword = 'Olajide1965',\n",
    "    \n",
    "    # Role for s3 \n",
    "    IamRoles = [RoleARN]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjiyeAdunoluwa\\AppData\\Local\\Temp\\ipykernel_26788\\4079417802.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth',-1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>youtube-api-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>awsuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DBName</td>\n",
       "      <td>flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'youtube-api-2023.clisu1wzhgfq.us-east-1.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-04fe5451242d56083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  MasterUsername      \n",
       "3  DBName              \n",
       "4  Endpoint            \n",
       "5  VpcId               \n",
       "\n",
       "                                                                                         Value  \n",
       "0  youtube-api-2023                                                                             \n",
       "1  dc2.large                                                                                    \n",
       "2  awsuser                                                                                      \n",
       "3  flight                                                                                       \n",
       "4  {'Address': 'youtube-api-2023.clisu1wzhgfq.us-east-1.redshift.amazonaws.com', 'Port': 5439}  \n",
       "5  vpc-04fe5451242d56083                                                                        "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get valuable infor from Cluster \n",
    "\n",
    "def prettyredshift(props):\n",
    "    pd.set_option('display.max_colwidth',-1)\n",
    "    keysToShow = [\"ClusterIdentifier\",\"NodeType\",\"Endpoint\",\"VpcId\",\"MasterUsername\",\"DBName\"] \n",
    "    x = [(k,v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data = x, columns =[\"Key\",\"Value\"])\n",
    "\n",
    "MyClusterProps = Redshift.describe_clusters(ClusterIdentifier = DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyredshift(MyClusterProps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('youtube-api-2023.clisu1wzhgfq.us-east-1.redshift.amazonaws.com', 'flight')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_ENPOINT = MyClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = MyClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "DB_NAME = MyClusterProps['DBName']\n",
    "DB_USER = MyClusterProps['MasterUsername']\n",
    "\n",
    "DWH_ENPOINT,DB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'EC2' object has no attribute 'Vpc'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id = MyClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName = defaultSg.group_name,\n",
    "        CidrIp = '0.0.0.0/0',\n",
    "        IpProtocl ='TCP',\n",
    "        FromPort = int(DWH_PORT),\n",
    "        Toport = int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the AWSRedshift database...\n",
      "Database Connected!!\n",
      "Load into table 'dimdate' failed.  Check 'stl_load_errors' system table for details.\n",
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "print('Connecting to the AWSRedshift database...')\n",
    "print('Database Connected!!')\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "                                host ='youtube-api-2023.clisu1wzhgfq.us-east-1.redshift.amazonaws.com',\n",
    "                                database = \"flight\",\n",
    "                                user = \"awsuser\",\n",
    "                                password = \"Olajide_1965\",\n",
    "                                port = 5439\n",
    "                                )\n",
    "    cur = conn.cursor()\n",
    "    conn.set_session(autocommit=True)\n",
    "        # Create an empty table\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "    CREATE TABLE \"FactCovid\" (\n",
    "    \"index\" INTEGER,\n",
    "    \"fips\" INTEGER,\n",
    "    \"province_state\" TEXT,\n",
    "    \"country_region\" TEXT,\n",
    "    \"confirmed\" REAL,\n",
    "    \"recovered\" REAL,\n",
    "    \"active\" REAL,\n",
    "    \"state\" TEXT,\n",
    "    \"date\" TEXT,\n",
    "    \"cases\" INTEGER,\n",
    "    \"deaths\" INTEGER);\n",
    "                \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "    CREATE TABLE \"DimRegion\" (\n",
    "    \"index\" INTEGER,\n",
    "    \"fips\" INTEGER,\n",
    "    \"province_state\" TEXT,\n",
    "    \"country_region\" TEXT,\n",
    "    \"latitude\" REAL,\n",
    "    \"longitude\" REAL,\n",
    "    \"county\" TEXT,\n",
    "    \"state\" TEXT);\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE \"DimDate\" (\n",
    "    \"index\" INTEGER,\n",
    "    \"fips\" INTEGER,\n",
    "    \"date\" TEXT\n",
    ")\n",
    "    \"\"\"   \n",
    ")\n",
    "\n",
    "\n",
    "# Copy Data from the transformed s3 Buckets to Redshift \n",
    "    # cur.execute(\"\"\"\n",
    "    #             COPY DimDate from 's3://analytics-buckets/Output/DimDate.csv'\n",
    "    #                iam_role 'arn:aws:iam::807724986505:role/new-s3-redshift'\n",
    "    #                 IGNOREHEADER 1\n",
    "    #                 csv;\n",
    "    #                 \"\"\")\n",
    "\n",
    "# cur.execute(\"\"\" \n",
    "#             select * from DimDate; \n",
    "#             \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "                COPY FactCovid from 's3://analytics-buckets/Output/FactCovid.csv'\n",
    "                    iam_role 'arn:aws:iam::807724986505:role/new-s3-redshift'\n",
    "                    IGNOREHEADER 1\n",
    "                    csv;\n",
    "                    \"\"\")\n",
    "    \n",
    "    # cur.execute(\"\"\"\n",
    "    #             SELECT *  FROM FactCovid;\n",
    "    #             \"\"\")\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "                COPY DimRegion from 's3://analytics-buckets/Output/DimRegion.csv'\n",
    "                    iam_role 'arn:aws:iam::807724986505:role/new-s3-redshift'\n",
    "                    IGNOREHEADER 1\n",
    "                    csv;\n",
    "                    \"\"\")\n",
    "    # cur.execute(\"\"\"\n",
    "    #             SELECT *  FROM DimRegion;\n",
    "    #             \"\"\")\n",
    "    # cur.execute(\"\"\"\n",
    "    #            SELECT *  FROM stl_load_errors where query = '1111';\n",
    "    #             \"\"\")\n",
    "    conn.commit()\n",
    "    conn.set_session(autocommit=True)\n",
    "\n",
    " # close the communication with the Redshift\n",
    "    cur.close()\n",
    "except (Exception, redshift_connector.DatabaseError) as error:\n",
    "        print(error)\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "cursor already closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AjiyeAdunoluwa\\Desktop\\Data Engineering Projects\\AWS Project 1\\Redshift.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mcreate view loadview as\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m(select distinct tbl, trim(name) as table_name, query, starttime,\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mtrim(filename) as input, line_number, colname, err_code,\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mtrim(err_reason) as reason\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom stl_load_errors sl, stv_tbl_perm sp\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwhere sl.tbl = sp.id);\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AjiyeAdunoluwa/Desktop/Data%20Engineering%20Projects/AWS%20Project%201/Redshift.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "\u001b[1;31mInterfaceError\u001b[0m: cursor already closed"
     ]
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "create view loadview as\n",
    "(select distinct tbl, trim(name) as table_name, query, starttime,\n",
    "trim(filename) as input, line_number, colname, err_code,\n",
    "trim(err_reason) as reason\n",
    "from stl_load_errors sl, stv_tbl_perm sp\n",
    "where sl.tbl = sp.id);\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f5784fe7dd7096e87533c7b3f94173ba36f17d15bb50207cac5c3801d115cff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
